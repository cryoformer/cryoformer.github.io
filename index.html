<!DOCTYPE html>
<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>CryoFormer</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <!-- mirror: F0%9F%AA%9E&lt -->
    <link rel="icon" type="image/png" href="img_cryoformer/protein_1.png">
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/font-awesome.min.css">
    <link rel="stylesheet" href="css/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <script src="js/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/codemirror.min.js"></script>
    <script src="js/clipboard.min.js"></script>
    <script src="js/video_comparison.js"></script>
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="header" style="text-align: center; margin: auto;">
        <div class="row" id="title-row" style="max-width: 100%; margin: 0 auto; display: inline-block">
            <h2 class="col-md-12 text-center" id="title">
                <b>CryoFormer</b>: Continuous Reconstruction of 3D Structures <br> from Cryo-EM Data using Transformer-based Neural Representations<br>
            </h2>
        </div>
                <div class="author-list">
                    <div class="author">
                        <a href="https://xinhangliu.com">Xinhang Liu</a>
                    </div>
                    <div class="author">
                        Yan Zeng
                    </div>
                    <div class="author">
                        Yifan Qin
                    </div>
                    <div class="author">
                        Hao Li
                    </div>
                    
                    <div class="author">
                        <a href="https://jiakai-zhang.github.io/">Jiakai Zhang</a>
                    </div>
                    <div class="author">
                        <a href="http:/xu-lan.com/">Lan Xu</a>
                    </div>
                    <div class="author">
                        <a href="https://sist.shanghaitech.edu.cn/sist_en/2020/0814/c7582a54832/page.htm">Jingyi Yu</a>
                    </div>
                </div>
    </div>
    <script>
        document.getElementById('author-row').style.maxWidth = document.getElementById("title-row").clientWidth + 'px';
    </script>
    <div class="container" id="main">
        <div class="row">
                <div class="col-sm-8 col-sm-offset-2 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://cryoformer.github.io/" target="_blank">
                                <img src="./img_cryoformer/file.png" height="60px"><br>
                                <strong>Paper<br>(coming soon)</strong>
                            </a>
                          </li>                        
                        <li>
                            <a href="https://cryoformer.github.io/" target="_blank">
                                <img src="./img_cryoformer/video-player.png" height="60px"><br>
                                <strong>Video<br>(coming soon)</strong>
                            </a>
                        </li>
                        <li>
                            <a href="https://cryoformer.github.io/" target="_blank">
                                <img src="./img_cryoformer/protein.png" height="60px"><br>
                              <strong>Dataset<br>(coming soon)</strong>
                            </a>
                          </li>                        
                        <li>
                            <a href="https://cryoformer.github.io/" target="_blank">
                                <img src="./img_cryoformer/code.png" height="60px"><br>
                                <strong>Code<br>(coming soon)</strong>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>



     

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    High-resolution heterogeneous reconstruction of 3D structures 
                    of proteins and other biomolecules using cryo-electron
                    microscopy (cryo-EM) is essential for understanding fundamental processes of life. 
                    However, it is still challenging
                    to reconstruct the continuous motions of 3D structures
                    from hundreds of thousands of noisy and randomly ori-
                    ented 2D cryo-EM images. Existing methods based on
                    coordinate-based neural networks show compelling results
                    to model continuous conformations of 3D structures in
                    the Fourier domain, but they suffer from a limited ability
                    to model local flexible regions and lack interpretability.
                    We propose a novel approach, cryoFormer, that utilizes
                    a transformer-based network architecture for continuous
                    heterogeneous cryo-EM reconstruction. We for the first
                    time directly reconstruct continuous conformations of 3D
                    structures using an implicit feature volume in the 3D spatial
                    domain. A novel deformation transformer decoder further
                    improves reconstruction quality and, more importantly,
                    locates and robustly tackles flexible 3D regions caused by
                    conformations. In experiments, our method outperforms
                    current approaches on three public datasets (1 synthetic
                    and 2 experimental) and a new synthetic dataset of PEDV
                    spike protein. The code and new synthetic dataset will be
                    released for better reproducibility of our results.
                </p>
            </div>
        </div>

        <image src="img_cryoformer/arch.png" class="img-responsive" alt="overview" width="60%" style="max-height: 450px;margin:auto;">

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://youtube.com/embed/qrdRH9irAlk" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Reflection Direction Parameterization
                </h3>
                <div class="text-justify">
                    Previous approaches directly input the camera's view direction into the MLP to predict outgoing radiance. We show that instead using the reflection of the view direction about the normal makes the emittance function significantly easier to learn and interpolate, greatly improving our results.
                    
                    <br><br>
                    
                </div>
                <div class="text-center">
                    <video id="refdir" width="40%" playsinline autoplay loop muted>
                        <source src="video/reflection_animation.mp4" type="video/mp4" />
                    </video>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Integrated Directional Encoding
                </h3>
                <div class="text-justify">
                    We explicitly model object roughness using the expected values of a set of spherical harmonics under a von Mises-Fisher distribution whose concentration parameter varies spatially:
                </div>
                <br>
                <div class="text-center">
                    <img src="./img/ide.png" width="50%">
                </div>
                <br>
                <div class="text-justify">
                    We call this <i>Integrated Directional Encoding</i>, and we show experimentally that it allows sharing the emittance functions between points with different roughnesses. It also enables scene editing after training. Theoretically, our encoding is stationary on the sphere, similar to the Euclidean stationarity of NeRF's positional encoding. <br><br>
                </div>
                <div class="text-center">
                    <video id="ide" width="100%" playsinline autoplay controls loop muted>
                        <source src="video/ide_animation.mp4" type="video/mp4" />
                    </video>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Additional Synthetic Results
                </h3>
                <div class="video-compare-container">
                    <video class="video" id="musclecar" loop playsinline autoPlay muted src="video/musclecar_mipnerf_ours.mp4" onplay="resizeAndPlay(this)"></video>
                    <canvas height=0 class="videoMerge" id="musclecarMerge"></canvas>
                </div>
			</div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results on Captured Scenes
                </h3>
                Our method also produces accurate renderings and surface normals from captured photographs:
                <div class="video-compare-container" style="width: 100%">
                    <video class="video" id="toycar" loop playsinline autoPlay muted src="video/toycar.mp4" onplay="resizeAndPlay(this)"></video>
                    <canvas height=0 class="videoMerge" id="toycarMerge"></canvas>
                </div>
			</div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Scene Editing
                </h3>
                <div class="text-justify">
                    We show that our structured representation of the directional MLP allows for scene editing after training. Here we show that we can convincingly change material properties.
                    <br>
                    We can increase and decrease material roughness:
                </div>

                <div style="overflow: hidden;">
                    <video id="editing-materials" width="100%" playsinline autoplay loop muted style="margin-top: -5%;">
                        <source src="video/materials_rougher_smoother.mp4" type="video/mp4" />
                    </video>
                </div>

                <div class="text-justify">
                    We can also control the amounts of specular and diffuse colors, or change the diffuse color without affecting the specular reflections:
                </div>
                
                <table width="100%">
                    <tr>
                        <td align="left" valign="top" width="50%">
                            <video id="v2" width="100%" playsinline autoplay loop muted>
                                <source src="video/car_color2.mp4" type="video/mp4" />
                            </video>
                        </td>
                        <td align="left" valign="top" width="50%">
                            <video id="v3" width="100%" playsinline autoplay loop muted>
                                <source src="video/car_color3.mp4" type="video/mp4" />
                            </video>
                        </td>
                    </tr>
                </table>

            </div>
        </div>

            
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@article{verbin2022refnerf,
    title={{Ref-NeRF}: Structured View-Dependent Appearance for
           Neural Radiance Fields},
    author={Dor Verbin and Peter Hedman and Ben Mildenhall and
            Todd Zickler and Jonathan T. Barron and Pratul P. Srinivasan},
    journal={CVPR},
    year={2022}
}</textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                We would like to thank Lior Yariv and Kai Zhang for helping us evaluate their methods, and Ricardo Martin-Brualla for helpful comments on our text. DV is supported by the National Science Foundation under Cooperative Agreement PHY-2019786 (an NSF AI Institute, <a href="http://iaifi.org">http://iaifi.org</a>)
                    <br>
                The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a>.
                </p>
            </div>
        </div>
    </div>


</body></html>
